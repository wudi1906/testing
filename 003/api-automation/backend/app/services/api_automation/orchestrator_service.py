"""
æ¥å£è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ç¼–æ’æœåŠ¡ - é‡æ–°è®¾è®¡ç‰ˆæœ¬
è´Ÿè´£åè°ƒå„ä¸ªæ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼Œä½¿ç”¨æ–°çš„æ•°æ®æ¨¡å‹
"""
from typing import Dict, List, Any, Optional
from datetime import datetime

from autogen_core import SingleThreadedAgentRuntime, TopicId
# from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime  # ä¸´æ—¶æ³¨é‡Šæ‰ï¼Œé¿å…grpcç‰ˆæœ¬å†²çª
from loguru import logger

from app.core.agents.collector import StreamResponseCollector
from app.core.types import AgentPlatform, TopicTypes
from app.core.enums import LogLevel
from app.agents.factory import agent_factory

# å¯¼å…¥é‡æ–°è®¾è®¡çš„æ•°æ®æ¨¡å‹
from app.agents.api_automation.schemas import (
    DocumentParseInput, DocumentFormat,
    AnalysisInput, TestCaseGenerationInput, ScriptGenerationInput
)

# å¯¼å…¥åŸºç¡€æ¶ˆæ¯ç±»å‹
from app.core.messages.base import BaseMessage
from pydantic import Field


# ç®€å•çš„æ—¥å¿—è®°å½•æ¶ˆæ¯ç±»å‹
class LogRecordRequest(BaseMessage):
    """æ—¥å¿—è®°å½•è¯·æ±‚"""
    agent_name: str = Field(..., description="æ™ºèƒ½ä½“åç§°")
    log_level: str = Field(..., description="æ—¥å¿—çº§åˆ«")
    log_message: str = Field(..., description="æ—¥å¿—æ¶ˆæ¯")
    log_data: Dict[str, Any] = Field(default_factory=dict, description="æ—¥å¿—æ•°æ®")
    execution_context: Dict[str, Any] = Field(default_factory=dict, description="æ‰§è¡Œä¸Šä¸‹æ–‡")


class ApiAutomationOrchestrator:
    """
    æ¥å£è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ç¼–æ’å™¨ - é‡æ–°è®¾è®¡ç‰ˆæœ¬

    è´Ÿè´£åè°ƒä»¥ä¸‹æ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼š
    1. APIæ–‡æ¡£è§£ææ™ºèƒ½ä½“ - è§£æAPIæ–‡æ¡£ï¼Œè¾“å‡º DocumentParseOutput
    2. æ¥å£åˆ†ææ™ºèƒ½ä½“ - åˆ†ææ¥å£ä¾èµ–å…³ç³»ï¼Œè¾“å‡º AnalysisOutput
    3. æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“ - ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè¾“å‡º TestCaseGenerationOutput
    4. è„šæœ¬ç”Ÿæˆæ™ºèƒ½ä½“ - ç”Ÿæˆpytestæµ‹è¯•è„šæœ¬ï¼Œè¾“å‡º ScriptGenerationOutput
    5. æ—¥å¿—è®°å½•æ™ºèƒ½ä½“ - è®°å½•æ‰§è¡Œæ—¥å¿—

    æ•°æ®æµè½¬ï¼šDocumentParseInput â†’ AnalysisInput â†’ TestCaseGenerationInput â†’ ScriptGenerationInput
    """

    def __init__(self, collector: Optional[StreamResponseCollector] = None):
        """
        åˆå§‹åŒ–æ¥å£è‡ªåŠ¨åŒ–ç¼–æ’å™¨
        
        Args:
            collector: å¯é€‰çš„StreamResponseCollectorç”¨äºæ•è·æ™ºèƒ½ä½“å“åº”
        """
        self.response_collector = collector or StreamResponseCollector(
            platform=AgentPlatform.API_AUTOMATION
        )
        self.runtime: Optional[SingleThreadedAgentRuntime] = None
        self.agent_factory = agent_factory
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
        # ç¼–æ’å™¨æ€§èƒ½æŒ‡æ ‡
        self.orchestrator_metrics = {
            "total_workflows": 0,
            "successful_workflows": 0,
            "failed_workflows": 0,
            "active_sessions": 0
        }
        
        logger.info("æ¥å£è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")

    async def initialize(self, **agent_kwargs) -> None:
        """
        åˆå§‹åŒ–ç¼–æ’å™¨å’Œæ™ºèƒ½ä½“
        
        Args:
            **agent_kwargs: æ™ºèƒ½ä½“åˆå§‹åŒ–å‚æ•°
        """
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–æ¥å£è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ç¼–æ’å™¨...")
            
            if self.runtime is None:
                # å¦‚æœæ˜¯åˆ†å¸ƒå¼è¿è¡Œæ—¶
                # self.runtime = GrpcWorkerAgentRuntime(host_address="localhost:50051")
                # åˆ›å»ºè¿è¡Œæ—¶

                self.runtime = SingleThreadedAgentRuntime()
                
                # æ³¨å†Œæ™ºèƒ½ä½“åˆ°è¿è¡Œæ—¶
                await self.agent_factory.register_agents_to_runtime(self.runtime)
                
                # è®¾ç½®å“åº”æ”¶é›†å™¨
                await self.agent_factory.register_stream_collector(
                    runtime=self.runtime,
                    collector=self.response_collector
                )
                
                # å¯åŠ¨è¿è¡Œæ—¶
                self.runtime.start()
                
                logger.info("âœ… æ¥å£è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ æ¥å£è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def process_api_document(
        self, 
        session_id: str,
        file_path: str,
        file_name: str,
        file_content: Optional[str] = None,
        doc_format: str = "auto",
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†APIæ–‡æ¡£çš„å®Œæ•´æµç¨‹
        
        Args:
            session_id: ä¼šè¯ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼‰
            doc_format: æ–‡æ¡£æ ¼å¼
            config: é…ç½®å‚æ•°
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        try:
            self.orchestrator_metrics["total_workflows"] += 1
            self.orchestrator_metrics["active_sessions"] += 1
            
            # è®°å½•ä¼šè¯ä¿¡æ¯
            self.active_sessions[session_id] = {
                "start_time": datetime.now(),
                "status": "processing",
                "current_step": "document_parsing",
                "file_name": file_name,
                "config": config or {}
            }
            
            logger.info(f"å¼€å§‹å¤„ç†APIæ–‡æ¡£: {file_name} (ä¼šè¯: {session_id})")
            
            # è®°å½•å¼€å§‹æ—¥å¿—
            await self._log_workflow_event(
                session_id, 
                "workflow_started", 
                f"å¼€å§‹å¤„ç†APIæ–‡æ¡£: {file_name}",
                {"file_path": file_path, "doc_format": doc_format}
            )
            
            # æ­¥éª¤1: è§£æAPIæ–‡æ¡£
            await self._parse_api_document(
                session_id, file_path, file_name, file_content, doc_format, config
            )
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            self.active_sessions[session_id]["current_step"] = "completed"
            self.active_sessions[session_id]["status"] = "completed"
            self.active_sessions[session_id]["end_time"] = datetime.now()
            
            self.orchestrator_metrics["successful_workflows"] += 1
            self.orchestrator_metrics["active_sessions"] -= 1
            
            # è®°å½•å®Œæˆæ—¥å¿—
            await self._log_workflow_event(
                session_id,
                "workflow_completed",
                f"APIæ–‡æ¡£å¤„ç†å®Œæˆ: {file_name}",
                {"duration": (datetime.now() - self.active_sessions[session_id]["start_time"]).total_seconds()}
            )
            
            return {
                "success": True,
                "session_id": session_id,
                "message": "APIæ–‡æ¡£å¤„ç†å®Œæˆ",
                "session_info": self.active_sessions[session_id],
                "note": "å®Œæ•´çš„å·¥ä½œæµç¨‹åŒ…æ‹¬ï¼šæ–‡æ¡£è§£æ â†’ æ¥å£åˆ†æ â†’ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ â†’ è„šæœ¬ç”Ÿæˆ"
            }
            
        except Exception as e:
            self.orchestrator_metrics["failed_workflows"] += 1
            self.orchestrator_metrics["active_sessions"] -= 1
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            if session_id in self.active_sessions:
                self.active_sessions[session_id]["status"] = "failed"
                self.active_sessions[session_id]["error"] = str(e)
                self.active_sessions[session_id]["end_time"] = datetime.now()
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            await self._log_workflow_event(
                session_id,
                "workflow_failed",
                f"APIæ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}",
                {"error": str(e), "file_name": file_name}
            )
            
            logger.error(f"å¤„ç†APIæ–‡æ¡£å¤±è´¥: {str(e)}")
            raise

    async def _parse_api_document(
        self,
        session_id: str,
        file_path: str,
        file_name: str,
        file_content: Optional[str],
        doc_format: str,
        config: Optional[Dict[str, Any]]
    ) -> None:
        """å‘é€APIæ–‡æ¡£è§£æè¯·æ±‚ - ä½¿ç”¨æ–°çš„æ•°æ®æ¨¡å‹"""
        try:
            # æ£€æµ‹æ–‡æ¡£æ ¼å¼
            detected_format = DocumentFormat.AUTO
            if doc_format.lower() in [fmt.value for fmt in DocumentFormat]:
                detected_format = DocumentFormat(doc_format.lower())

            # æ„å»ºè§£æè¯·æ±‚ - ä½¿ç”¨æ–°çš„æ•°æ®æ¨¡å‹
            parse_request = DocumentParseInput(
                session_id=session_id,
                file_path=file_path,
                file_name=file_name,
                file_content=file_content,
                doc_format=detected_format,
                parse_options=config or {}
            )

            # å‘é€åˆ°APIæ–‡æ¡£è§£ææ™ºèƒ½ä½“
            await self.runtime.publish_message(
                parse_request,
                topic_id=TopicId(type=TopicTypes.API_DOC_PARSER.value, source="orchestrator")
            )

            logger.info(f"å·²å‘é€APIæ–‡æ¡£è§£æè¯·æ±‚: {session_id}")
            logger.debug(f"è§£æè¯·æ±‚è¯¦æƒ…: æ–‡ä»¶={file_name}, æ ¼å¼={detected_format.value}")

        except Exception as e:
            logger.error(f"å‘é€APIæ–‡æ¡£è§£æè¯·æ±‚å¤±è´¥: {str(e)}")
            raise

    async def execute_test_suite(
        self,
        session_id: str,
        script_files: List[str],
        test_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæµ‹è¯•å¥—ä»¶ - é€‚é…æ–°çš„æ•°æ®æµ

        æ³¨æ„ï¼šåœ¨æ–°çš„æ¶æ„ä¸­ï¼Œæµ‹è¯•è„šæœ¬æ˜¯ç”±è„šæœ¬ç”Ÿæˆæ™ºèƒ½ä½“è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œ
        è¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºæ‰‹åŠ¨æ‰§è¡Œå·²ç”Ÿæˆçš„æµ‹è¯•è„šæœ¬ã€‚

        Args:
            session_id: ä¼šè¯ID
            script_files: æµ‹è¯•è„šæœ¬æ–‡ä»¶åˆ—è¡¨
            test_config: æµ‹è¯•é…ç½®

        Returns:
            Dict[str, Any]: æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œæµ‹è¯•å¥—ä»¶: {session_id}")

            # åœ¨æ–°æ¶æ„ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ‰§è¡Œè¯·æ±‚
            # æˆ–è€…ç›´æ¥è°ƒç”¨æµ‹è¯•æ‰§è¡Œå™¨
            execution_config = test_config or {
                "framework": "pytest",
                "parallel": False,
                "max_workers": 1,
                "timeout": 300,
                "report_formats": ["allure", "html"]
            }

            # è®°å½•æ‰§è¡Œæ—¥å¿—
            await self._log_workflow_event(
                session_id,
                "test_execution_started",
                f"å¼€å§‹æ‰§è¡Œæµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å« {len(script_files)} ä¸ªè„šæœ¬æ–‡ä»¶",
                {"script_files": script_files, "config": execution_config}
            )

            # TODO: åœ¨æ–°æ¶æ„ä¸­ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®¾è®¡æµ‹è¯•æ‰§è¡Œçš„æ¶ˆæ¯æ¨¡å‹
            # ç›®å‰å…ˆè¿”å›æˆåŠŸçŠ¶æ€ï¼Œå®é™…æ‰§è¡Œé€»è¾‘éœ€è¦æ ¹æ®æ–°çš„æ•°æ®æ¨¡å‹è°ƒæ•´
            logger.warning("æµ‹è¯•æ‰§è¡ŒåŠŸèƒ½éœ€è¦æ ¹æ®æ–°çš„æ•°æ®æ¨¡å‹é‡æ–°å®ç°")

            return {
                "success": True,
                "session_id": session_id,
                "message": "æµ‹è¯•æ‰§è¡ŒåŠŸèƒ½æ­£åœ¨é€‚é…æ–°çš„æ•°æ®æ¨¡å‹",
                "script_count": len(script_files),
                "note": "æ­¤åŠŸèƒ½éœ€è¦é‡æ–°å®ç°ä»¥é€‚é…æ–°çš„æ™ºèƒ½ä½“æ¶æ„"
            }

        except Exception as e:
            logger.error(f"æ‰§è¡Œæµ‹è¯•å¥—ä»¶å¤±è´¥: {str(e)}")
            await self._log_workflow_event(
                session_id,
                "test_execution_failed",
                f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}",
                {"error": str(e)}
            )
            raise

    async def get_session_status(self, session_id: str) -> Dict[str, Any]:
        """
        è·å–ä¼šè¯çŠ¶æ€
        
        Args:
            session_id: ä¼šè¯ID
            
        Returns:
            Dict[str, Any]: ä¼šè¯çŠ¶æ€ä¿¡æ¯
        """
        try:
            if session_id not in self.active_sessions:
                return {
                    "success": False,
                    "message": "ä¼šè¯ä¸å­˜åœ¨",
                    "session_id": session_id
                }
            
            session_info = self.active_sessions[session_id].copy()
            
            # æ·»åŠ è¿è¡Œæ—¶é—´
            if "start_time" in session_info:
                if session_info.get("status") == "processing":
                    session_info["running_time"] = (
                        datetime.now() - session_info["start_time"]
                    ).total_seconds()
                elif "end_time" in session_info:
                    session_info["total_time"] = (
                        session_info["end_time"] - session_info["start_time"]
                    ).total_seconds()
            
            return {
                "success": True,
                "session_id": session_id,
                "session_info": session_info
            }
            
        except Exception as e:
            logger.error(f"è·å–ä¼šè¯çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": str(e),
                "session_id": session_id
            }

    async def get_orchestrator_metrics(self) -> Dict[str, Any]:
        """è·å–ç¼–æ’å™¨æŒ‡æ ‡"""
        try:
            # è·å–æ™ºèƒ½ä½“å¥åº·çŠ¶æ€
            agent_health = await self.agent_factory.health_check_all()
            
            # è·å–æ€§èƒ½æ‘˜è¦
            performance_summary = await self.agent_factory.get_performance_summary()
            
            return {
                "orchestrator_metrics": self.orchestrator_metrics,
                "agent_health": agent_health,
                "performance_summary": performance_summary,
                "active_sessions_count": len(self.active_sessions),
                "active_sessions": list(self.active_sessions.keys()),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–ç¼–æ’å™¨æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def _log_workflow_event(
        self,
        session_id: str,
        event_type: str,
        message: str,
        data: Dict[str, Any]
    ) -> None:
        """è®°å½•å·¥ä½œæµäº‹ä»¶æ—¥å¿—"""
        try:
            # ä½¿ç”¨loguruç›´æ¥è®°å½•æ—¥å¿—ï¼Œé¿å…ä¾èµ–æ—¥å¿—è®°å½•æ™ºèƒ½ä½“
            logger.info(
                f"[{session_id}] {event_type}: {message}",
                extra={
                    "session_id": session_id,
                    "event_type": event_type,
                    "event_data": data,
                    "orchestrator": "api_automation"
                }
            )

            # å¦‚æœéœ€è¦å‘é€åˆ°æ—¥å¿—è®°å½•æ™ºèƒ½ä½“ï¼Œå¯ä»¥å°è¯•å‘é€ï¼ˆä½†ä¸å¼ºåˆ¶è¦æ±‚æˆåŠŸï¼‰
            try:
                if self.runtime and hasattr(TopicTypes, 'LOG_RECORDER'):
                    log_request = LogRecordRequest(
                        session_id=session_id,
                        agent_name="ApiAutomationOrchestrator",
                        log_level="INFO",
                        log_message=message,
                        log_data=data,
                        execution_context={
                            "event_type": event_type,
                            "orchestrator": "api_automation"
                        }
                    )

                    await self.runtime.publish_message(
                        log_request,
                        topic_id=TopicId(type=TopicTypes.LOG_RECORDER.value, source="orchestrator")
                    )
            except Exception as inner_e:
                # æ—¥å¿—è®°å½•æ™ºèƒ½ä½“ä¸å¯ç”¨æ—¶ï¼Œä¸å½±å“ä¸»æµç¨‹
                logger.debug(f"å‘é€åˆ°æ—¥å¿—è®°å½•æ™ºèƒ½ä½“å¤±è´¥ï¼ˆéå…³é”®é”™è¯¯ï¼‰: {str(inner_e)}")

        except Exception as e:
            logger.error(f"è®°å½•å·¥ä½œæµäº‹ä»¶å¤±è´¥: {str(e)}")

    async def cleanup(self) -> None:
        """æ¸…ç†ç¼–æ’å™¨èµ„æº"""
        try:
            # æ¸…ç†æ™ºèƒ½ä½“
            await self.agent_factory.cleanup_all()
            
            # æ¸…ç†å“åº”æ”¶é›†å™¨
            if self.response_collector:
                self.response_collector.cleanup()
            
            # åœæ­¢è¿è¡Œæ—¶
            if self.runtime:
                self.runtime.stop()
            
            # æ¸…ç†ä¼šè¯
            self.active_sessions.clear()
            
            logger.info("æ¥å£è‡ªåŠ¨åŒ–ç¼–æ’å™¨èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼–æ’å™¨èµ„æºå¤±è´¥: {str(e)}")

    def get_factory_status(self) -> Dict[str, Any]:
        """è·å–å·¥å‚çŠ¶æ€"""
        return self.agent_factory.get_factory_status()

    async def run_complete_workflow(
        self,
        session_id: str,
        file_path: str,
        file_name: str,
        file_content: Optional[str] = None,
        doc_format: str = "auto",
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„APIè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹

        è¿™ä¸ªæ–¹æ³•å¯åŠ¨å®Œæ•´çš„å·¥ä½œæµï¼š
        æ–‡æ¡£è§£æ â†’ æ¥å£åˆ†æ â†’ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ â†’ è„šæœ¬ç”Ÿæˆ

        Args:
            session_id: ä¼šè¯ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼‰
            doc_format: æ–‡æ¡£æ ¼å¼
            config: é…ç½®å‚æ•°

        Returns:
            Dict[str, Any]: å·¥ä½œæµå¯åŠ¨ç»“æœ
        """
        try:
            logger.info(f"ğŸš€ å¯åŠ¨å®Œæ•´çš„APIè‡ªåŠ¨åŒ–å·¥ä½œæµ: {file_name}")

            # è®°å½•å·¥ä½œæµå¼€å§‹
            self.active_sessions[session_id] = {
                "start_time": datetime.now(),
                "status": "running_complete_workflow",
                "current_step": "document_parsing",
                "file_name": file_name,
                "config": config or {},
                "workflow_type": "complete"
            }

            # å¯åŠ¨æ–‡æ¡£è§£æï¼ˆè¿™å°†è§¦å‘æ•´ä¸ªå·¥ä½œæµé“¾ï¼‰
            await self._parse_api_document(
                session_id, file_path, file_name, file_content, doc_format, config
            )

            # è®°å½•å·¥ä½œæµå¯åŠ¨æ—¥å¿—
            await self._log_workflow_event(
                session_id,
                "complete_workflow_started",
                f"å®Œæ•´å·¥ä½œæµå·²å¯åŠ¨: {file_name}",
                {
                    "file_path": file_path,
                    "doc_format": doc_format,
                    "workflow_steps": [
                        "document_parsing",
                        "api_analysis",
                        "test_case_generation",
                        "script_generation"
                    ]
                }
            )

            return {
                "success": True,
                "session_id": session_id,
                "message": "å®Œæ•´çš„APIè‡ªåŠ¨åŒ–å·¥ä½œæµå·²å¯åŠ¨",
                "workflow_steps": [
                    "1. æ–‡æ¡£è§£æ - æå–APIç«¯ç‚¹ä¿¡æ¯",
                    "2. æ¥å£åˆ†æ - åˆ†æä¾èµ–å…³ç³»å’Œæ‰§è¡Œé¡ºåº",
                    "3. æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ - ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹",
                    "4. è„šæœ¬ç”Ÿæˆ - ç”Ÿæˆå¯æ‰§è¡Œçš„pytestè„šæœ¬"
                ],
                "note": "å·¥ä½œæµå°†è‡ªåŠ¨åœ¨æ™ºèƒ½ä½“ä¹‹é—´ä¼ é€’æ•°æ®ï¼Œè¯·é€šè¿‡ get_session_status ç›‘æ§è¿›åº¦"
            }

        except Exception as e:
            logger.error(f"å¯åŠ¨å®Œæ•´å·¥ä½œæµå¤±è´¥: {str(e)}")
            await self._log_workflow_event(
                session_id,
                "complete_workflow_failed",
                f"å®Œæ•´å·¥ä½œæµå¯åŠ¨å¤±è´¥: {str(e)}",
                {"error": str(e)}
            )
            raise

    async def generate_interface_script(
        self,
        session_id: str,
        interface_obj,  # ç›´æ¥ä¼ é€’æ•°æ®åº“å¯¹è±¡
        document_obj    # ç›´æ¥ä¼ é€’æ–‡æ¡£å¯¹è±¡
    ) -> Dict[str, Any]:
        """
        ä¸ºå•ä¸ªæ¥å£ç”Ÿæˆæµ‹è¯•è„šæœ¬

        Args:
            session_id: ä¼šè¯ID
            interface_obj: æ¥å£æ•°æ®åº“å¯¹è±¡ (ApiInterface)
            document_obj: æ–‡æ¡£æ•°æ®åº“å¯¹è±¡ (ApiDocument)

        Returns:
            Dict[str, Any]: ç”Ÿæˆç»“æœ
        """
        try:
            # ä»æ•°æ®åº“å¯¹è±¡ä¸­æå–åŸºæœ¬ä¿¡æ¯
            interface_id = interface_obj.interface_id
            document_id = document_obj.doc_id

            logger.info(f"ğŸš€ å¼€å§‹ä¸ºæ¥å£ç”Ÿæˆè„šæœ¬: interface_id={interface_id}")

            # è®°å½•ä¼šè¯ä¿¡æ¯
            self.active_sessions[session_id] = {
                "session_id": session_id,
                "document_id": document_id,
                "interface_id": interface_id,
                "workflow_type": "interface_script_generation",
                "status": "processing",
                "current_step": "analysis",
                "started_at": datetime.now().isoformat(),
                "interface_name": interface_obj.name,
                "interface_path": f"{interface_obj.method} {interface_obj.path}"
            }

            # æ›´æ–°æŒ‡æ ‡
            self.orchestrator_metrics["total_workflows"] += 1
            self.orchestrator_metrics["active_sessions"] += 1

            # è®°å½•å·¥ä½œæµäº‹ä»¶
            await self._log_workflow_event(
                session_id,
                "interface_script_generation_started",
                f"å¼€å§‹ä¸ºæ¥å£ {interface_id} ç”Ÿæˆè„šæœ¬",
                {"interface_id": interface_id, "document_id": document_id}
            )

            # æ„å»ºåˆ†æè¾“å…¥
            from app.agents.api_automation.schemas import (
                AnalysisInput, ParsedApiInfo, ParsedEndpoint,
                ApiParameter, ApiResponse, ParameterLocation, DataType
            )

            # ç›´æ¥ä»æ–‡æ¡£å¯¹è±¡æ„å»ºAPIä¿¡æ¯ - åªä½¿ç”¨ParsedApiInfoä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
            parsed_api_info = ParsedApiInfo(
                title=document_obj.api_info.get("title", "API") if document_obj.api_info else "API",
                version=document_obj.api_info.get("version", "1.0") if document_obj.api_info else "1.0",
                description=document_obj.api_info.get("description", "") if document_obj.api_info else "",
                base_url=document_obj.api_info.get("base_url", "") if document_obj.api_info else "",
                contact=document_obj.api_info.get("contact", {}) if document_obj.api_info else {},
                license=document_obj.api_info.get("license", {}) if document_obj.api_info else {}
            )

            # ç›´æ¥ä»æ¥å£å¯¹è±¡æ„å»ºå‚æ•°ä¿¡æ¯
            parameters = []
            for param in interface_obj.parameters:
                # æ„å»ºå‚æ•°çº¦æŸä¿¡æ¯
                constraints = {}
                if hasattr(param, 'constraints') and param.constraints:
                    constraints = param.constraints
                else:
                    # ä»å…¶ä»–å­—æ®µæ„å»ºçº¦æŸä¿¡æ¯
                    if hasattr(param, 'format') and param.format:
                        constraints['format'] = param.format
                    if hasattr(param, 'pattern') and param.pattern:
                        constraints['pattern'] = param.pattern
                    if hasattr(param, 'min_length') and param.min_length is not None:
                        constraints['min_length'] = param.min_length
                    if hasattr(param, 'max_length') and param.max_length is not None:
                        constraints['max_length'] = param.max_length
                    if hasattr(param, 'minimum') and param.minimum is not None:
                        constraints['minimum'] = param.minimum
                    if hasattr(param, 'maximum') and param.maximum is not None:
                        constraints['maximum'] = param.maximum

                parameters.append(ApiParameter(
                    name=param.name,
                    location=ParameterLocation(param.location),
                    data_type=DataType(param.data_type),
                    required=param.required,
                    description=param.description or "",
                    example=param.example,
                    constraints=constraints
                ))

            # ç›´æ¥ä»æ¥å£å¯¹è±¡æ„å»ºå“åº”ä¿¡æ¯
            responses = []
            for resp in interface_obj.responses:
                responses.append(ApiResponse(
                    status_code=resp.status_code,
                    description=resp.description or "",
                    content_type=resp.content_type or "application/json",
                    response_schema=resp.response_schema or {},
                    example=resp.example
                ))

            # ç›´æ¥ä»æ¥å£å¯¹è±¡æ„å»ºå®Œæ•´çš„ç«¯ç‚¹å¯¹è±¡
            parsed_endpoint = ParsedEndpoint(
                endpoint_id=interface_obj.endpoint_id or interface_obj.interface_id,
                path=interface_obj.path,
                method=interface_obj.method,
                summary=interface_obj.summary or "",
                description=interface_obj.description or "",
                parameters=parameters,
                responses=responses,
                tags=interface_obj.tags or [],
                auth_required=interface_obj.auth_required,
                deprecated=interface_obj.is_deprecated,
                # ç›´æ¥ä»æ•°æ®åº“å¯¹è±¡è·å–æ‰©å±•ä¿¡æ¯
                extended_info=interface_obj.extended_info or {},
                raw_data=interface_obj.raw_data or {},
                security_schemes=interface_obj.security_schemes or {},
                complexity_score=interface_obj.complexity_score,
                confidence_score=interface_obj.confidence_score,
                interface_name=interface_obj.name,
                category=interface_obj.category or "",
                auth_type=interface_obj.auth_type or ""
            )

            # æ„å»ºåˆ†æè¾“å…¥ï¼ŒåŒ…å«ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            analysis_options = {
                "interface_id": interface_id,
                "single_interface_mode": True,  # æ ‡è¯†è¿™æ˜¯å•æ¥å£è„šæœ¬ç”Ÿæˆ
                "generation_focus": "comprehensive",  # ç”Ÿæˆå…¨é¢çš„æµ‹è¯•è„šæœ¬
                "include_edge_cases": True,  # åŒ…å«è¾¹ç•Œæƒ…å†µæµ‹è¯•
                "include_error_handling": True,  # åŒ…å«é”™è¯¯å¤„ç†æµ‹è¯•
                "use_extended_info": True,  # ä½¿ç”¨æ‰©å±•ä¿¡æ¯
                # ç›´æ¥ä»æ•°æ®åº“å¯¹è±¡è·å–æ–‡æ¡£çº§åˆ«çš„ä¿¡æ¯
                "document_format": document_obj.doc_format,
                "document_version": document_obj.doc_version,
                "api_title": interface_obj.api_title,
                "api_version": interface_obj.api_version,
                # ç›´æ¥ä»æ¥å£å¯¹è±¡è·å–è´¨é‡è¯„ä¼°ä¿¡æ¯
                "complexity_score": interface_obj.complexity_score,
                "confidence_score": interface_obj.confidence_score,
                # ç›´æ¥ä»æ¥å£å¯¹è±¡è·å–æ‰©å±•ä¿¡æ¯çš„å…³é”®å­—æ®µ
                "extended_info": interface_obj.extended_info,
                "raw_data": interface_obj.raw_data,
                "auth_type": interface_obj.auth_type,
                "category": interface_obj.category,
                # é¢å¤–çš„æ¥å£ä¿¡æ¯
                "interface_name": interface_obj.name,
                "base_url": interface_obj.base_url
            }

            analysis_input = AnalysisInput(
                session_id=session_id,
                document_id=document_id,
                interface_id=interface_id,  # ä¼ é€’interface_id
                api_info=parsed_api_info,
                endpoints=[parsed_endpoint],
                analysis_options=analysis_options
            )

            # å‘é€åˆ°æ¥å£åˆ†ææ™ºèƒ½ä½“
            await self.runtime.publish_message(
                analysis_input,
                topic_id=TopicId(type=TopicTypes.API_ANALYZER.value, source="orchestrator")
            )

            logger.info(f"âœ… æ¥å£è„šæœ¬ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨: {interface_id}")

            return {
                "success": True,
                "session_id": session_id,
                "interface_id": interface_id,
                "message": "æ¥å£è„šæœ¬ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨",
                "status": "processing"
            }

        except Exception as e:
            # æ›´æ–°æŒ‡æ ‡
            self.orchestrator_metrics["failed_workflows"] += 1
            if session_id in self.active_sessions:
                self.active_sessions[session_id]["status"] = "failed"
                self.active_sessions[session_id]["error"] = str(e)
                self.active_sessions[session_id]["failed_at"] = datetime.now().isoformat()

            # è®°å½•é”™è¯¯äº‹ä»¶
            await self._log_workflow_event(
                session_id,
                "interface_script_generation_failed",
                f"æ¥å£è„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}",
                {"error": str(e), "interface_id": interface_id}
            )

            logger.error(f"âŒ æ¥å£è„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise

    async def trigger_analysis_step(
        self,
        session_id: str,
        document_id: str,
        api_info: Dict[str, Any],
        endpoints: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ‰‹åŠ¨è§¦å‘æ¥å£åˆ†ææ­¥éª¤

        è¿™ä¸ªæ–¹æ³•å¯ä»¥ç”¨äºæµ‹è¯•æˆ–æ‰‹åŠ¨æ§åˆ¶å·¥ä½œæµ
        """
        try:
            # æ„å»ºåˆ†æè¾“å…¥ï¼ˆéœ€è¦æ ¹æ®å®é™…çš„æ•°æ®ç»“æ„è°ƒæ•´ï¼‰
            # è¿™é‡Œéœ€è¦å°†å­—å…¸è½¬æ¢ä¸ºç›¸åº”çš„æ•°æ®æ¨¡å‹å¯¹è±¡
            logger.info(f"æ‰‹åŠ¨è§¦å‘æ¥å£åˆ†ææ­¥éª¤: {session_id}")

            # TODO: å®ç°æ•°æ®è½¬æ¢é€»è¾‘
            logger.warning("æ‰‹åŠ¨è§¦å‘åˆ†ææ­¥éª¤åŠŸèƒ½éœ€è¦å®Œå–„æ•°æ®è½¬æ¢é€»è¾‘")

            return {
                "success": True,
                "session_id": session_id,
                "message": "æ¥å£åˆ†ææ­¥éª¤è§¦å‘åŠŸèƒ½éœ€è¦å®Œå–„",
                "note": "éœ€è¦å®ç°ä»å­—å…¸åˆ°æ•°æ®æ¨¡å‹çš„è½¬æ¢é€»è¾‘"
            }

        except Exception as e:
            logger.error(f"è§¦å‘æ¥å£åˆ†ææ­¥éª¤å¤±è´¥: {str(e)}")
            raise

    def get_workflow_status(self, session_id: str) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµçŠ¶æ€ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰

        æä¾›æ›´è¯¦ç»†çš„å·¥ä½œæµè¿›åº¦ä¿¡æ¯
        """
        try:
            if session_id not in self.active_sessions:
                return {
                    "success": False,
                    "message": "ä¼šè¯ä¸å­˜åœ¨",
                    "session_id": session_id
                }

            session_info = self.active_sessions[session_id].copy()

            # æ·»åŠ å·¥ä½œæµè¿›åº¦ä¿¡æ¯
            if session_info.get("workflow_type") == "complete":
                workflow_steps = [
                    {"step": "document_parsing", "name": "æ–‡æ¡£è§£æ", "status": "completed" if session_info.get("current_step") != "document_parsing" else "running"},
                    {"step": "api_analysis", "name": "æ¥å£åˆ†æ", "status": "pending"},
                    {"step": "test_case_generation", "name": "æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ", "status": "pending"},
                    {"step": "script_generation", "name": "è„šæœ¬ç”Ÿæˆ", "status": "pending"}
                ]

                # æ ¹æ®å½“å‰æ­¥éª¤æ›´æ–°çŠ¶æ€
                current_step = session_info.get("current_step", "")
                for i, step in enumerate(workflow_steps):
                    if step["step"] == current_step:
                        step["status"] = "running"
                        # æ ‡è®°ä¹‹å‰çš„æ­¥éª¤ä¸ºå·²å®Œæˆ
                        for j in range(i):
                            workflow_steps[j]["status"] = "completed"
                        break

                session_info["workflow_progress"] = workflow_steps
                session_info["progress_percentage"] = (
                    len([s for s in workflow_steps if s["status"] == "completed"]) / len(workflow_steps) * 100
                )

            return {
                "success": True,
                "session_id": session_id,
                "session_info": session_info
            }

        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµçŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": str(e),
                "session_id": session_id
            }
