import os
import sys
import uuid
import json
import hashlib
from typing import Optional, Dict, Any, List
from pathlib import Path
from datetime import datetime
from fastapi import UploadFile, HTTPException

# æ·»åŠ componentsç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from components.config import MarkerConfig
from components.file_processor import FileProcessor


class DocumentService:
    def __init__(self, upload_dir: str = "uploads"):
        self.upload_dir = Path(upload_dir)
        self.upload_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæ–‡ä»¶å­˜å‚¨ç›®å½•
        self.files_db_dir = self.upload_dir / "files_db"
        self.files_db_dir.mkdir(exist_ok=True)

        # å†…å­˜ä¸­çš„æ–‡ä»¶å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
        self.uploaded_files: Dict[str, Dict[str, Any]] = {}
        self.session_files: Dict[str, List[str]] = {}  # session_id -> [file_ids]

        # æ–‡ä»¶è§£æç¼“å­˜ - åŸºäºæ–‡ä»¶å†…å®¹å“ˆå¸Œ
        self.file_cache: Dict[str, Dict[str, Any]] = {}  # file_hash -> parsed_content

        # åˆ›å»ºmarkeré…ç½®
        self.config = MarkerConfig(
            output_format="markdown",
            use_llm=self._should_use_llm(),
            format_lines=True,
            enable_image_description=True,  # å¯ç”¨å›¾ç‰‡æè¿°åŠŸèƒ½
            disable_image_extraction=self._should_use_llm(),  # ä½¿ç”¨LLMæ—¶å¯ç”¨å›¾ç‰‡æè¿°
            max_file_size=50 * 1024 * 1024,  # 50MB
            qwen_api_key=os.getenv("QWEN_API_KEY", ""),
            openai_api_key=os.getenv("OPENAI_API_KEY", ""),
            # ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
            openai_model="gpt-4o" if os.getenv("OPENAI_API_KEY") else "gpt-4o-mini",
            qwen_model="qwen-vl-max-latest",
        )

        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        self.file_processor = FileProcessor(
            config=self.config,
            upload_dir=upload_dir
        )

        # åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶ä¿¡æ¯
        self._load_files_db()

    def _should_use_llm(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨LLM"""
        return bool(os.getenv("QWEN_API_KEY") or os.getenv("OPENAI_API_KEY"))

    def _calculate_file_hash(self, file_content: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(file_content).hexdigest()

    def _get_cached_content(self, file_hash: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„æ–‡ä»¶å†…å®¹"""
        return self.file_cache.get(file_hash)

    def _cache_file_content(self, file_hash: str, filename: str, content: str, markdown_content: Dict[str, Any]) -> None:
        """ç¼“å­˜æ–‡ä»¶å†…å®¹"""
        cache_entry = {
            "filename": filename,
            "content": content,
            "markdown_content": markdown_content,
            "cached_time": datetime.now().isoformat(),
            "file_hash": file_hash
        }
        self.file_cache[file_hash] = cache_entry

        # é™åˆ¶ç¼“å­˜å¤§å°ï¼ˆä¿ç•™æœ€è¿‘çš„100ä¸ªæ–‡ä»¶ï¼‰
        if len(self.file_cache) > 100:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_hash = min(self.file_cache.keys(),
                            key=lambda h: self.file_cache[h].get("cached_time", ""))
            del self.file_cache[oldest_hash]

    def _load_files_db(self):
        """åŠ è½½æ–‡ä»¶æ•°æ®åº“"""
        db_file = self.files_db_dir / "files.json"
        if db_file.exists():
            try:
                with open(db_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.uploaded_files = data.get('uploaded_files', {})
                    self.session_files = data.get('session_files', {})
                    self.file_cache = data.get('file_cache', {})
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {e}")

        # åŠ è½½æ–‡ä»¶ç¼“å­˜
        cache_file = self.files_db_dir / "file_cache.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.file_cache = json.load(f)
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")

    def _save_files_db(self):
        """ä¿å­˜æ–‡ä»¶æ•°æ®åº“"""
        db_file = self.files_db_dir / "files.json"
        try:
            with open(db_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'uploaded_files': self.uploaded_files,
                    'session_files': self.session_files,
                    'file_cache': self.file_cache
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {e}")

        # å•ç‹¬ä¿å­˜æ–‡ä»¶ç¼“å­˜ï¼ˆé¿å…ä¸»æ•°æ®åº“è¿‡å¤§ï¼‰
        cache_file = self.files_db_dir / "file_cache.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")

    async def save_and_extract_file(self, file: UploadFile, session_id: str = "default") -> Dict[str, Any]:
        """ä¿å­˜æ–‡ä»¶å¹¶ä½¿ç”¨markeræå–å†…å®¹ï¼Œè¿”å›æ–‡ä»¶ID"""
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹è®¡ç®—å“ˆå¸Œ
            file_content = await file.read()
            file_hash = self._calculate_file_hash(file_content)

            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            await file.seek(0)

            # æ£€æŸ¥ç¼“å­˜
            cached_content = self._get_cached_content(file_hash)

            if cached_content:
                print(f"âœ… æ–‡ä»¶å·²è§£æè¿‡ï¼Œä½¿ç”¨ç¼“å­˜: {file.filename}")

                # ä½¿ç”¨ç¼“å­˜çš„å†…å®¹
                plain_text = cached_content["content"]
                markdown_content_data = cached_content["markdown_content"]

                # ç”Ÿæˆæ–‡ä»¶ID
                file_id = str(uuid.uuid4())

                # ä»éœ€ä¿å­˜ç‰©ç†æ–‡ä»¶ï¼ˆç”¨äºåç»­å¯èƒ½çš„éœ€æ±‚ï¼‰
                file_extension = Path(file.filename).suffix.lower()
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                file_path = self.upload_dir / unique_filename

                with open(file_path, 'wb') as f:
                    f.write(file_content)

                result = {
                    "filename": file.filename,
                    "saved_filename": unique_filename,
                    "file_path": str(file_path),
                    "file_size": len(file_content),
                    "file_type": file_extension,
                    "markdown_content": markdown_content_data,
                    "processing_config": {"cached": True}
                }
            else:
                print(f"ğŸ”„ é¦–æ¬¡è§£ææ–‡ä»¶: {file.filename}")

                # ä½¿ç”¨æ–°çš„æ–‡ä»¶å¤„ç†å™¨å¤„ç†ä¸Šä¼ æ–‡ä»¶
                result = await self.file_processor.process_upload_file(file)

                # ç”Ÿæˆæ–‡ä»¶ID
                file_id = str(uuid.uuid4())

                # æå–markdownå†…å®¹ä¸­çš„çº¯æ–‡æœ¬ç”¨äºèŠå¤©
                markdown_content_data = result["markdown_content"]
                plain_text = markdown_content_data.get("plain_text", "")

                # ç¼“å­˜è§£æç»“æœ
                self._cache_file_content(file_hash, file.filename, plain_text, markdown_content_data)

            # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
            file_info = {
                "file_id": file_id,
                "filename": result["filename"],
                "saved_filename": result["saved_filename"],
                "file_path": result["file_path"],
                "content": plain_text,  # ç”¨äºèŠå¤©çš„çº¯æ–‡æœ¬å†…å®¹
                "file_size": result["file_size"],
                "file_type": result["file_type"],
                "markdown_content": result["markdown_content"],
                "processing_config": result.get("processing_config", {}),
                "upload_time": datetime.now().isoformat(),
                "session_id": session_id
            }

            # ä¿å­˜åˆ°å†…å­˜å­˜å‚¨
            self.uploaded_files[file_id] = file_info

            # å…³è”åˆ°ä¼šè¯
            if session_id not in self.session_files:
                self.session_files[session_id] = []
            self.session_files[session_id].append(file_id)

            # æŒä¹…åŒ–å­˜å‚¨
            self._save_files_db()

            # è¿”å›æ–‡ä»¶IDå’ŒåŸºæœ¬ä¿¡æ¯ï¼ˆä¸åŒ…å«å†…å®¹ï¼‰
            return {
                "file_id": file_id,
                "filename": result["filename"],
                "file_size": result["file_size"],
                "file_type": result["file_type"],
                "upload_time": file_info["upload_time"],
                "statistics": {
                    "total_characters": markdown_content_data["statistics"]["total_characters"],
                    "total_words": markdown_content_data["statistics"]["total_words"],
                    "tables_count": markdown_content_data["statistics"]["tables_count"],
                    "images_count": markdown_content_data["statistics"]["images_count"],
                    "headers_count": markdown_content_data["statistics"]["headers_count"]
                },
                "processing_info": {
                    "llm_enabled": result.get("processing_config", {}).get("use_llm", False),
                    "format_enhanced": result.get("processing_config", {}).get("format_lines", False)
                }
            }

        except HTTPException:
            # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
            raise
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")

    def get_file_content(self, file_id: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶IDè·å–æ–‡ä»¶å†…å®¹"""
        if file_id in self.uploaded_files:
            return self.uploaded_files[file_id]["content"]
        return None

    def get_file_info_by_id(self, file_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®æ–‡ä»¶IDè·å–æ–‡ä»¶ä¿¡æ¯"""
        return self.uploaded_files.get(file_id)

    def get_session_files(self, session_id: str) -> List[Dict[str, Any]]:
        """è·å–ä¼šè¯å…³è”çš„æ‰€æœ‰æ–‡ä»¶"""
        file_ids = self.session_files.get(session_id, [])
        files = []
        for file_id in file_ids:
            if file_id in self.uploaded_files:
                file_info = self.uploaded_files[file_id].copy()
                # ä¸è¿”å›å®Œæ•´å†…å®¹ï¼Œåªè¿”å›åŸºæœ¬ä¿¡æ¯
                file_info.pop("content", None)
                file_info.pop("markdown_content", None)
                files.append(file_info)
        return files

    def get_session_content(self, session_id: str) -> str:
        """è·å–ä¼šè¯æ‰€æœ‰æ–‡ä»¶çš„åˆå¹¶å†…å®¹"""
        file_ids = self.session_files.get(session_id, [])
        contents = []

        for file_id in file_ids:
            if file_id in self.uploaded_files:
                file_info = self.uploaded_files[file_id]
                content = file_info["content"]
                filename = file_info["filename"]
                contents.append(f"=== æ–‡ä»¶: {filename} ===\n{content}\n")

        return "\n".join(contents)

    def remove_file(self, file_id: str, session_id: str = None) -> bool:
        """åˆ é™¤æ–‡ä»¶"""
        if file_id not in self.uploaded_files:
            return False

        file_info = self.uploaded_files[file_id]

        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        try:
            file_path = Path(file_info["file_path"])
            if file_path.exists():
                file_path.unlink()
        except Exception as e:
            print(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {e}")

        # ä»å†…å­˜ä¸­åˆ é™¤
        del self.uploaded_files[file_id]

        # ä»ä¼šè¯ä¸­ç§»é™¤
        if session_id and session_id in self.session_files:
            if file_id in self.session_files[session_id]:
                self.session_files[session_id].remove(file_id)

        # ä»æ‰€æœ‰ä¼šè¯ä¸­ç§»é™¤
        for sid, file_list in self.session_files.items():
            if file_id in file_list:
                file_list.remove(file_id)

        # æŒä¹…åŒ–
        self._save_files_db()
        return True

    def clear_session_files(self, session_id: str) -> int:
        """æ¸…é™¤ä¼šè¯çš„æ‰€æœ‰æ–‡ä»¶"""
        if session_id not in self.session_files:
            return 0

        file_ids = self.session_files[session_id].copy()
        removed_count = 0

        for file_id in file_ids:
            if self.remove_file(file_id, session_id):
                removed_count += 1

        return removed_count

    async def process_file_direct(self, file_path: str) -> Dict[str, Any]:
        """ç›´æ¥å¤„ç†æ–‡ä»¶è·¯å¾„"""
        try:
            markdown_content = await self.file_processor.process_file(file_path)
            structured_content = self.file_processor.markdown_extractor.get_structured_content(markdown_content)

            return {
                "status": "success",
                "file_path": file_path,
                "content": structured_content["plain_text"],
                "markdown_content": structured_content
            }
        except Exception as e:
            return {
                "status": "error",
                "file_path": file_path,
                "error": str(e)
            }

    async def search_in_document(self, file_path: str, query: str, case_sensitive: bool = False) -> List[Dict[str, Any]]:
        """åœ¨æ–‡æ¡£ä¸­æœç´¢å†…å®¹"""
        try:
            markdown_content = await self.file_processor.process_file(file_path)
            return self.file_processor.search_in_content(markdown_content, query, case_sensitive)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")

    def get_supported_formats(self) -> Dict[str, Any]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
        return {
            "supported_formats": self.file_processor.get_supported_formats(),
            "max_file_size_mb": self.config.max_file_size / 1024 / 1024,
            "llm_enabled": self.config.use_llm
        }

    def delete_file(self, file_path: str) -> bool:
        """åˆ é™¤æ–‡ä»¶"""
        return self.file_processor.delete_file(file_path)

    def get_file_info(self, file_path: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        return self.file_processor.get_file_info(file_path)

    def update_config(self, **kwargs) -> None:
        """æ›´æ–°å¤„ç†é…ç½®"""
        self.file_processor.update_config(**kwargs)
        # åŒæ­¥æ›´æ–°æœ¬åœ°é…ç½®
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)

    def get_processing_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰å¤„ç†é…ç½®"""
        return self.file_processor.get_config()

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_size": len(self.file_cache),
            "cache_entries": [
                {
                    "file_hash": file_hash[:8] + "...",  # åªæ˜¾ç¤ºå‰8ä½
                    "filename": entry["filename"],
                    "cached_time": entry["cached_time"],
                    "content_length": len(entry["content"])
                }
                for file_hash, entry in self.file_cache.items()
            ]
        }

    def clear_cache(self) -> int:
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        cache_count = len(self.file_cache)
        self.file_cache.clear()
        self._save_files_db()
        return cache_count

    def remove_cache_entry(self, file_hash: str) -> bool:
        """åˆ é™¤ç‰¹å®šçš„ç¼“å­˜é¡¹"""
        if file_hash in self.file_cache:
            del self.file_cache[file_hash]
            self._save_files_db()
            return True
        return False

    def is_file_cached(self, file_content: bytes) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¼“å­˜"""
        file_hash = self._calculate_file_hash(file_content)
        return file_hash in self.file_cache
