"""
Webé¡µé¢åˆ†æ - é¡µé¢æˆªå›¾æ™ºèƒ½åˆ†æAPI
æ”¯æŒSSEæµå¼æ¥å£å’Œé¡µé¢å…ƒç´ è¯†åˆ«
"""
from autogen_core import CancellationToken, MessageContext, ClosureContext
from fastapi import APIRouter, Request, Depends, HTTPException, BackgroundTasks, File, UploadFile, Form
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
import asyncio
import logging
import uuid
import json
import base64
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.messages import StreamMessage
from app.core.messages.web import WebMultimodalAnalysisRequest
from app.database.connection import db_manager
from app.database.repositories.page_analysis_repository import PageAnalysisRepository, PageElementRepository

router = APIRouter()

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# ä¼šè¯å­˜å‚¨
active_sessions: Dict[str, Dict[str, Any]] = {}

# æ¶ˆæ¯é˜Ÿåˆ—å­˜å‚¨
message_queues: Dict[str, asyncio.Queue] = {}

# ä¼šè¯è¶…æ—¶ï¼ˆç§’ï¼‰
SESSION_TIMEOUT = 3600  # 1å°æ—¶


async def get_db_session():
    """è·å–æ•°æ®åº“ä¼šè¯"""
    try:
        logger.info("æ­£åœ¨è·å–æ•°æ®åº“ä¼šè¯...")
        async with db_manager.get_session() as session:
            logger.info("æ•°æ®åº“ä¼šè¯è·å–æˆåŠŸ")
            yield session
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“ä¼šè¯å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise


async def cleanup_session(session_id: str, delay: int = SESSION_TIMEOUT):
    """åœ¨æŒ‡å®šå»¶è¿Ÿåæ¸…ç†ä¼šè¯èµ„æº"""
    await asyncio.sleep(delay)
    if session_id in active_sessions:
        logger.info(f"æ¸…ç†è¿‡æœŸä¼šè¯: {session_id}")
        active_sessions.pop(session_id, None)
        message_queues.pop(session_id, None)


@router.get("/health")
async def health_check():
    """é¡µé¢åˆ†æå¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "ok", "service": "web-page-analysis", "timestamp": datetime.now().isoformat()}


@router.post("/upload-and-analyze")
async def upload_and_analyze_pages(
    files: List[UploadFile] = File(...),
    description: Optional[str] = Form(None),
    page_url: Optional[str] = Form(None),
    page_name: Optional[str] = Form(None)
):
    """
    ä¸Šä¼ é¡µé¢æˆªå›¾å¹¶å¯åŠ¨AIåˆ†æä»»åŠ¡
    
    Args:
        files: ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        description: é¡µé¢æè¿°
        page_url: é¡µé¢URLï¼ˆå¯é€‰ï¼‰
        page_name: é¡µé¢åç§°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        Dict: åŒ…å«session_idçš„å“åº”
    """
    try:
        # éªŒè¯æ–‡ä»¶
        if not files:
            raise HTTPException(status_code=400, detail="è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶")
        
        # éªŒè¯æ¯ä¸ªæ–‡ä»¶
        validated_files = []
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            if not file.content_type or not file.content_type.startswith('image/'):
                raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ {file.filename} ä¸æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶")
            
            # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆ10MBé™åˆ¶ï¼‰
            content = await file.read()
            file_size = len(content)
            if file_size > 10 * 1024 * 1024:  # 10MB
                raise HTTPException(status_code=400, detail=f"å›¾ç‰‡æ–‡ä»¶ {file.filename} å¤§å°ä¸èƒ½è¶…è¿‡10MB")
            
            # è½¬æ¢ä¸ºbase64
            image_base64 = base64.b64encode(content).decode('utf-8')
            
            validated_files.append({
                "filename": file.filename,
                "content_type": file.content_type,
                "size": file_size,
                "image_data": image_base64
            })
        
        # ç”Ÿæˆä¼šè¯ID
        session_id = str(uuid.uuid4())

        # è®°å½•å½“å‰æ—¶é—´
        current_time = datetime.now()

        # ä¸è®¾ç½®é»˜è®¤å€¼ï¼Œä¿æŒç”¨æˆ·è¾“å…¥çš„åŸå§‹å€¼ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
        final_page_name = page_name.strip() if page_name else None
        final_description = description.strip() if description else None

        # å­˜å‚¨ä¼šè¯ä¿¡æ¯
        active_sessions[session_id] = {
            "status": "processing",  # ç›´æ¥è®¾ç½®ä¸ºå¤„ç†ä¸­
            "created_at": current_time.isoformat(),
            "last_activity": current_time.isoformat(),
            "files": validated_files,
            "page_info": {
                "description": final_description,
                "page_url": page_url.strip() if page_url else None,
                "page_name": final_page_name
            },
            "analysis_results": [],
            "progress": 0,
            "total_files": len(validated_files),
            "processed_files": 0
        }

        # åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
        message_queue = asyncio.Queue()
        message_queues[session_id] = message_queue

        # ç«‹å³å¯åŠ¨åå°åˆ†æä»»åŠ¡
        asyncio.create_task(process_page_analysis_task(session_id))

        logger.info(f"é¡µé¢åˆ†æä»»åŠ¡å·²åˆ›å»ºå¹¶å¯åŠ¨: {session_id}, æ–‡ä»¶æ•°é‡: {len(validated_files)}")

        return JSONResponse({
            "success": True,
            "message": "é¡µé¢åˆ†æä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°å¤„ç†",
            "data": {
                "session_id": session_id,
                "status": "processing",
                "uploaded_files": [f["filename"] for f in validated_files],
                "analysis_started": True,
                "sse_endpoint": f"/api/v1/web/page-analysis/stream/{session_id}",
                "status_endpoint": f"/api/v1/web/page-analysis/status/{session_id}",
                "files_info": [
                    {
                        "filename": f["filename"],
                        "size": f["size"]
                    } for f in validated_files
                ]
            }
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºé¡µé¢åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/stream/{session_id}")
async def stream_page_analysis(
    session_id: str,
    request: Request,
    background_tasks: BackgroundTasks
):
    """
    é¡µé¢åˆ†æSSEæµå¼ç«¯ç‚¹

    Args:
        session_id: ä¼šè¯ID
        request: HTTPè¯·æ±‚å¯¹è±¡
        background_tasks: åå°ä»»åŠ¡ç®¡ç†å™¨

    Returns:
        EventSourceResponse: SSEå“åº”æµ
    """
    # éªŒè¯ä¼šè¯æ˜¯å¦å­˜åœ¨
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    logger.info(f"å¼€å§‹é¡µé¢åˆ†æSSEæµ: {session_id}")

    # ç¡®ä¿æ¶ˆæ¯é˜Ÿåˆ—å­˜åœ¨
    if session_id not in message_queues:
        message_queue = asyncio.Queue()
        message_queues[session_id] = message_queue
        logger.info(f"åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—: {session_id}")
    else:
        message_queue = message_queues[session_id]
        logger.info(f"ä½¿ç”¨ç°æœ‰æ¶ˆæ¯é˜Ÿåˆ—: {session_id}")

    # è®¾ç½®ä¼šè¯è¶…æ—¶æ¸…ç†
    background_tasks.add_task(cleanup_session, session_id)
    
    # è¿”å›SSEå“åº”
    response = EventSourceResponse(
        page_analysis_event_generator(session_id, request),
        media_type="text/event-stream"
    )
    
    # æ·»åŠ å¿…è¦çš„å“åº”å¤´
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Connection"] = "keep-alive"
    response.headers["X-Accel-Buffering"] = "no"  # ç¦ç”¨Nginxç¼“å†²
    
    return response


async def page_analysis_event_generator(session_id: str, request: Request):
    """ç”Ÿæˆé¡µé¢åˆ†æSSEäº‹ä»¶æµ"""
    logger.info(f"å¼€å§‹ç”Ÿæˆé¡µé¢åˆ†æäº‹ä»¶æµ: {session_id}")
    
    # å‘é€ä¼šè¯åˆå§‹åŒ–äº‹ä»¶
    init_data = json.dumps({
        "session_id": session_id,
        "status": "connected",
        "service": "web_page_analysis"
    })
    yield f"event: session\nid: 0\ndata: {init_data}\n\n"
    
    # è·å–æ¶ˆæ¯é˜Ÿåˆ—
    message_queue = message_queues.get(session_id)
    if not message_queue:
        error_data = json.dumps({
            "error": "ä¼šè¯é˜Ÿåˆ—ä¸å­˜åœ¨"
        })
        yield f"event: error\nid: error-1\ndata: {error_data}\n\n"
        return
    
    # æ¶ˆæ¯IDè®¡æ•°å™¨
    message_id = 1
    
    try:
        # æŒç»­ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯å¹¶å‘é€
        while True:
            # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€è¿æ¥
            if await request.is_disconnected():
                logger.info(f"å®¢æˆ·ç«¯æ–­å¼€è¿æ¥: {session_id}")
                break
            
            # å°è¯•ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯ï¼ˆéé˜»å¡ï¼‰
            try:
                # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œç¡®ä¿æ›´é¢‘ç¹åœ°æ£€æŸ¥è¿æ¥çŠ¶æ€
                message = await asyncio.wait_for(message_queue.get(), timeout=0.5)
                
                # æ›´æ–°ä¼šè¯æœ€åæ´»åŠ¨æ—¶é—´
                if session_id in active_sessions:
                    active_sessions[session_id]["last_activity"] = datetime.now().isoformat()
                
                # ç¡®å®šäº‹ä»¶ç±»å‹
                event_type = message.type
                
                # å°†æ¶ˆæ¯è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                message_json = message.model_dump_json()
                
                logger.debug(f"å‘é€äº‹ä»¶: id={message_id}, type={event_type}, region={message.region}")
                
                # ä½¿ç”¨æ­£ç¡®çš„SSEæ ¼å¼å‘é€æ¶ˆæ¯
                yield f"event: {event_type}\nid: {message_id}\ndata: {message_json}\n\n"
                message_id += 1
                
                # å¦‚æœæ˜¯æœ€ç»ˆæ¶ˆæ¯ï¼Œç»§ç»­ä¿æŒè¿æ¥
                if message.is_final and event_type == "final_result":
                    logger.info(f"æ”¶åˆ°æœ€ç»ˆç»“æœï¼Œç»§ç»­ä¿æŒè¿æ¥: {session_id}")
                
            except asyncio.TimeoutError:
                # å‘é€ä¿æŒè¿æ¥çš„æ¶ˆæ¯
                ping_data = json.dumps({"timestamp": datetime.now().isoformat()})
                yield f"event: ping\nid: ping-{message_id}\ndata: {ping_data}\n\n"
                message_id += 1
                continue
                
    except Exception as e:
        logger.error(f"ç”Ÿæˆäº‹ä»¶æµæ—¶å‡ºé”™: {str(e)}")
        error_data = json.dumps({
            "error": f"ç”Ÿæˆäº‹ä»¶æµæ—¶å‡ºé”™: {str(e)}"
        })
        yield f"event: error\nid: error-{message_id}\ndata: {error_data}\n\n"
    
    # å‘é€å…³é—­äº‹ä»¶
    close_data = json.dumps({
        "message": "æµå·²å…³é—­"
    })
    logger.info(f"äº‹ä»¶æµç»“æŸ: {session_id}")
    yield f"event: close\nid: close-{message_id}\ndata: {close_data}\n\n"


async def process_page_analysis_task(session_id: str):
    """å¤„ç†é¡µé¢åˆ†æçš„åå°ä»»åŠ¡"""
    logger.info(f"å¼€å§‹æ‰§è¡Œé¡µé¢åˆ†æä»»åŠ¡: {session_id}")
    
    try:
        # è·å–æ¶ˆæ¯é˜Ÿåˆ—
        message_queue = message_queues.get(session_id)
        if not message_queue:
            logger.error(f"ä¼šè¯ {session_id} çš„æ¶ˆæ¯é˜Ÿåˆ—ä¸å­˜åœ¨")
            return

        # è·å–ä¼šè¯ä¿¡æ¯
        session_info = active_sessions.get(session_id)
        if not session_info:
            logger.error(f"ä¼šè¯ {session_id} ä¿¡æ¯ä¸å­˜åœ¨")
            return
        
        # ä¼šè¯çŠ¶æ€å·²åœ¨ä¸Šä¼ æ—¶è®¾ç½®ä¸ºprocessing

        # å‘é€å¼€å§‹æ¶ˆæ¯
        message = StreamMessage(
            message_id=f"system-{uuid.uuid4()}",
            type="message",
            source="ç³»ç»Ÿ",
            content="ğŸš€ å¼€å§‹é¡µé¢åˆ†ææµç¨‹...",
            region="process",
            platform="web",
            is_final=False,
        )
        await message_queue.put(message)


        # è®¾ç½®æ¶ˆæ¯å›è°ƒå‡½æ•°
        async def message_callback(ctx: ClosureContext, message: StreamMessage, message_ctx: MessageContext) -> None:
            try:
                # è·å–å½“å‰é˜Ÿåˆ—ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°çš„é˜Ÿåˆ—å¼•ç”¨ï¼‰
                current_queue = message_queues.get(session_id)
                if current_queue:
                    await current_queue.put(message)
                else:
                    logger.error(f"æ¶ˆæ¯å›è°ƒï¼šä¼šè¯ {session_id} çš„é˜Ÿåˆ—ä¸å­˜åœ¨")

            except Exception as e:
                logger.error(f"æ¶ˆæ¯å›è°ƒå¤„ç†é”™è¯¯: {str(e)}")

        # åˆ›å»ºå“åº”æ”¶é›†å™¨
        from app.core.agents import StreamResponseCollector
        from app.core.types import AgentPlatform
        collector = StreamResponseCollector(platform=AgentPlatform.WEB)
        collector.set_callback(message_callback)

        # è·å–Webç¼–æ’å™¨
        from app.services.web.orchestrator_service import get_web_orchestrator
        orchestrator = get_web_orchestrator(collector=collector)

        # è·å–é¡µé¢ä¿¡æ¯
        page_info = session_info["page_info"]
        files = session_info["files"]

        # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶
        for i, file_info in enumerate(files):
            try:
                # æ›´æ–°è¿›åº¦
                progress = int((i / len(files)) * 100)
                active_sessions[session_id]["progress"] = progress
                active_sessions[session_id]["processed_files"] = i
                active_sessions[session_id]["last_activity"] = datetime.now().isoformat()

                # å‘é€å¤„ç†è¿›åº¦æ¶ˆæ¯
                progress_message = StreamMessage(
                    message_id=f"progress-{uuid.uuid4()}",
                    type="message",
                    source="ç³»ç»Ÿ",
                    content=f"ğŸ“¸ æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(files)} ä¸ªé¡µé¢æˆªå›¾: {file_info['filename']} ({progress}%)",
                    region="process",
                    platform="web",
                    is_final=False,
                )
                await message_queue.put(progress_message)

                # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„åˆ†æIDï¼Œä½†ä¿æŒåŸå§‹session_id
                file_analysis_id = f"{session_id}_file_{i}"

                # ä½¿ç”¨ç¼–æ’å™¨æ‰§è¡Œé¡µé¢åˆ†æ
                await orchestrator.analyze_page_elements(
                    session_id=file_analysis_id,  # ä½¿ç”¨ç‹¬ç«‹çš„åˆ†æID
                    image_data=file_info["image_data"],
                    page_name=page_info.get("page_name", "") if page_info.get("page_name", "") else "",
                    page_description=page_info.get("description", "") if page_info.get("description", "") else "",
                    page_url=page_info.get("page_url", "")
                )

                # æ›´æ–°å®Œæˆçš„æ–‡ä»¶æ•°
                active_sessions[session_id]["processed_files"] = i + 1

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_info['filename']} å¤±è´¥: {str(e)}")
                # å‘é€é”™è¯¯æ¶ˆæ¯ä½†ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
                error_message = StreamMessage(
                    message_id=f"error-{uuid.uuid4()}",
                    type="message",
                    source="ç³»ç»Ÿ",
                    content=f"âŒ æ–‡ä»¶ {file_info['filename']} åˆ†æå¤±è´¥: {str(e)}",
                    region="process",
                    platform="web",
                    is_final=False,
                )
                await message_queue.put(error_message)
        
        # å‘é€æœ€ç»ˆç»“æœ
        final_message = StreamMessage(
            message_id=f"final-{uuid.uuid4()}",
            type="final_result",
            source="ç³»ç»Ÿ",
            content="âœ… é¡µé¢åˆ†ææµç¨‹å®Œæˆï¼Œåˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“",
            region="process",
            platform="web",
            is_final=True,
        )
        await message_queue.put(final_message)

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        active_sessions[session_id]["status"] = "completed"
        active_sessions[session_id]["progress"] = 100
        active_sessions[session_id]["processed_files"] = len(files)
        active_sessions[session_id]["completed_at"] = datetime.now().isoformat()
        active_sessions[session_id]["last_activity"] = datetime.now().isoformat()

        logger.info(f"é¡µé¢åˆ†æä»»åŠ¡å·²å®Œæˆ: {session_id}")
        
    except Exception as e:
        logger.error(f"é¡µé¢åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        
        # å‘é€é”™è¯¯æ¶ˆæ¯
        try:
            error_message = StreamMessage(
                message_id=f"error-{uuid.uuid4()}",
                type="error",
                source="ç³»ç»Ÿ",
                content=f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}",
                region="process",
                platform="web",
                is_final=True
            )

            message_queue = message_queues.get(session_id)
            if message_queue:
                await message_queue.put(error_message)
                
        except Exception as send_error:
            logger.error(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {str(send_error)}")
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        if session_id in active_sessions:
            active_sessions[session_id]["status"] = "error"
            active_sessions[session_id]["error"] = str(e)
            active_sessions[session_id]["error_at"] = datetime.now().isoformat()





@router.get("/sessions")
async def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰æ´»åŠ¨ä¼šè¯"""
    return JSONResponse({
        "sessions": active_sessions,
        "total": len(active_sessions)
    })


@router.get("/status/{session_id}")
async def get_analysis_status(session_id: str):
    """è·å–åˆ†æçŠ¶æ€"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    session_info = active_sessions[session_id]

    return JSONResponse({
        "success": True,
        "data": {
            "session_id": session_id,
            "status": session_info["status"],
            "progress": session_info.get("progress", 0),
            "total_files": session_info.get("total_files", 0),
            "processed_files": session_info.get("processed_files", 0),
            "created_at": session_info["created_at"],
            "last_activity": session_info["last_activity"],
            "error": session_info.get("error"),
            "completed_at": session_info.get("completed_at"),
            "page_info": session_info["page_info"]
        }
    })

@router.get("/sessions/{session_id}")
async def get_session(session_id: str):
    """è·å–æŒ‡å®šä¼šè¯çš„ä¿¡æ¯"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    return JSONResponse(active_sessions[session_id])


@router.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤æŒ‡å®šä¼šè¯"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    # åˆ é™¤ä¼šè¯èµ„æº
    active_sessions.pop(session_id, None)
    message_queues.pop(session_id, None)

    return JSONResponse({
        "status": "success",
        "message": f"ä¼šè¯ {session_id} å·²åˆ é™¤"
    })


@router.get("/pages")
async def get_page_list(
    page: int = 1,
    page_size: int = 20,
    search: Optional[str] = None,
    status: Optional[str] = None,
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–é¡µé¢åˆ—è¡¨"""
    try:
        logger.info(f"å¼€å§‹è·å–é¡µé¢åˆ—è¡¨ï¼Œé¡µç : {page}, é¡µé¢å¤§å°: {page_size}")

        repo = PageAnalysisRepository()

        if search:
            logger.info(f"æœç´¢é¡µé¢ï¼Œå…³é”®è¯: {search}")
            pages = await repo.search_by_page_name(session, search, limit=page_size)
        else:
            # è·å–æ‰€æœ‰é¡µé¢
            logger.info("è·å–æ‰€æœ‰é¡µé¢")
            from sqlalchemy import select, desc
            from app.database.models.page_analysis import PageAnalysisResult

            query = select(PageAnalysisResult).order_by(desc(PageAnalysisResult.created_at))
            result = await session.execute(query.limit(page_size).offset((page - 1) * page_size))
            pages = result.scalars().all()

        logger.info(f"æŸ¥è¯¢åˆ° {len(pages)} æ¡é¡µé¢è®°å½•")

        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        page_data = []
        for page_obj in pages:
            try:
                logger.debug(f"å¤„ç†é¡µé¢: {page_obj.id}")

                # ä½¿ç”¨æ”¹è¿›åçš„to_dictæ–¹æ³•
                page_dict = page_obj.to_dict()

                # æ·»åŠ åˆ†æçŠ¶æ€ï¼ˆåŸºäºç½®ä¿¡åº¦å’Œå…ƒç´ æ•°é‡æ¨æ–­ï¼‰
                elements_count = page_dict.get('elements_count', 0) or 0
                confidence_score = page_dict.get('confidence_score', 0) or 0

                if elements_count > 0 and confidence_score > 0:
                    page_dict['analysis_status'] = 'completed'
                elif elements_count == 0:
                    page_dict['analysis_status'] = 'pending'
                else:
                    page_dict['analysis_status'] = 'analyzing'

                page_data.append(page_dict)
                logger.debug(f"é¡µé¢ {page_obj.id} å¤„ç†æˆåŠŸ")

            except Exception as page_error:
                logger.error(f"å¤„ç†é¡µé¢ {page_obj.id} æ—¶å‡ºé”™: {page_error}")
                import traceback
                logger.error(f"é¡µé¢å¤„ç†è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„å®‰å…¨å­—å…¸
                safe_dict = {
                    "id": str(page_obj.id) if hasattr(page_obj, 'id') and page_obj.id else None,
                    "page_name": str(page_obj.page_name) if hasattr(page_obj, 'page_name') and page_obj.page_name else "æœªçŸ¥é¡µé¢",
                    "analysis_status": "error",
                    "error": "æ•°æ®å¤„ç†é”™è¯¯",
                    "confidence_score": 0.0,
                    "elements_count": 0,
                    "created_at": None,
                    "updated_at": None
                }
                page_data.append(safe_dict)

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "success": True,
            "data": page_data,
            "total": len(page_data),
            "page": page,
            "page_size": page_size
        }

        logger.info(f"æˆåŠŸè·å–é¡µé¢åˆ—è¡¨ï¼Œå…± {len(page_data)} æ¡è®°å½•")
        return response_data

    except Exception as e:
        logger.error(f"è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/pages/{page_id}/elements")
async def get_page_elements(
    page_id: str,
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–é¡µé¢å…ƒç´ åˆ—è¡¨"""
    try:
        element_repo = PageElementRepository()
        elements = await element_repo.get_by_analysis_id(session, page_id)
        
        element_data = [element.to_dict() for element in elements]
        
        return JSONResponse({
            "success": True,
            "data": element_data,
            "total": len(element_data)
        })
        
    except Exception as e:
        logger.error(f"è·å–é¡µé¢å…ƒç´ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–é¡µé¢å…ƒç´ å¤±è´¥")


@router.delete("/pages/{page_id}")
async def delete_page(
    page_id: str,
    session: AsyncSession = Depends(get_db_session)
):
    """åˆ é™¤é¡µé¢"""
    try:
        repo = PageAnalysisRepository()
        page = await repo.get_by_id(session, page_id)
        
        if not page:
            raise HTTPException(status_code=404, detail="é¡µé¢ä¸å­˜åœ¨")
        
        # åˆ é™¤é¡µé¢è®°å½•ï¼ˆçº§è”åˆ é™¤å…ƒç´ ï¼‰
        await session.delete(page)
        await session.commit()
        
        return JSONResponse({
            "success": True,
            "message": "é¡µé¢åˆ é™¤æˆåŠŸ"
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤é¡µé¢å¤±è´¥: {e}")
        await session.rollback()
        raise HTTPException(status_code=500, detail="åˆ é™¤é¡µé¢å¤±è´¥")
