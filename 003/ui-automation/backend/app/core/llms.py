"""
å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯é…ç½®
æ ¹æ®éªŒè¯ç»“æœä¼˜åŒ–æ¨¡å‹é€‰æ‹©å’Œä½¿ç”¨ï¼š
- QWen-VL: UIè‡ªåŠ¨åŒ–æœ€ä½³ï¼Œè§†è§‰ç†è§£ã€å›¾ç‰‡å¤„ç†
- GLM-4V: å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¤æ‚åœºæ™¯åˆ†æ  
- DeepSeek: ä»£ç ç”Ÿæˆã€æ–‡æœ¬å¤„ç†ï¼Œæ€§ä»·æ¯”æé«˜
"""
from typing import Optional, Dict, Any, List
from abc import ABC, abstractmethod

from openai import AsyncOpenAI

from autogen_ext.models.openai import OpenAIChatCompletionClient
from loguru import logger

from app.core.config import settings

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_deepseek_model_client = None
_qwenvl_model_client = None
_glm_model_client = None
_uitars_model_client = None

def get_deepseek_model_client() -> OpenAIChatCompletionClient:
    """è·å–DeepSeekå®¢æˆ·ç«¯ - ä¸“ç”¨äºä»£ç ç”Ÿæˆã€æ–‡æœ¬å¤„ç† (æ€§ä»·æ¯”æé«˜)"""
    global _deepseek_model_client
    if _deepseek_model_client is None:
        logger.info("ğŸ’° åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ - ä»£ç ç”Ÿæˆä¸“ç”¨")
        _deepseek_model_client = OpenAIChatCompletionClient(
            model=settings.DEEPSEEK_MODEL,
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL,
            model_info={
                "vision": False,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,
                "family": "deepseek",
                "multiple_system_messages": True
            }
        )
    return _deepseek_model_client

def get_qwenvl_model_client() -> OpenAIChatCompletionClient:
    """è·å–QWen-VLå®¢æˆ·ç«¯ - æœ€ä½³æ¨èï¼Œä¸“ç”¨äºUIè‡ªåŠ¨åŒ–ã€å›¾ç‰‡åˆ†æ"""
    global _qwenvl_model_client
    if _qwenvl_model_client is None:
        logger.info("ğŸ¥‡ åˆå§‹åŒ–QWen-VLå®¢æˆ·ç«¯ - UIè‡ªåŠ¨åŒ–æœ€ä½³é€‰æ‹©")
        _qwenvl_model_client = OpenAIChatCompletionClient(
            model=settings.QWEN_VL_MODEL,
            api_key=settings.QWEN_VL_API_KEY,
            base_url=settings.QWEN_VL_BASE_URL,
            model_info={
                "vision": True,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,
                "family": "qwen",
                "multiple_system_messages": True
            }
        )
    return _qwenvl_model_client

def get_glm_model_client() -> OpenAIChatCompletionClient:
    """è·å–GLM-4Vå®¢æˆ·ç«¯ - ä¸“ç”¨äºå¤æ‚å¤šæ¨¡æ€ä»»åŠ¡"""
    global _glm_model_client
    if _glm_model_client is None:
        logger.info("ğŸ¥ˆ åˆå§‹åŒ–GLM-4Vå®¢æˆ·ç«¯ - å¤šæ¨¡æ€ä»»åŠ¡ä¸“ç”¨")
        _glm_model_client = OpenAIChatCompletionClient(
            model=settings.GLM_MODEL,
            api_key=settings.GLM_API_KEY,
            base_url=settings.GLM_BASE_URL,
            model_info={
                "vision": True,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,
                "family": "glm",
                "multiple_system_messages": True
            }
        )
    return _glm_model_client

def get_uitars_model_client() -> OpenAIChatCompletionClient:
    """è·å–UI-TARSå®¢æˆ·ç«¯ - è±†åŒ…UIè‡ªåŠ¨åŒ–ä¸“ç”¨"""
    global _uitars_model_client
    if _uitars_model_client is None:
        try:
            logger.info("ğŸ¯ åˆå§‹åŒ–UI-TARSå®¢æˆ·ç«¯ - è±†åŒ…UIè‡ªåŠ¨åŒ–ä¸“ç”¨")
            _uitars_model_client = OpenAIChatCompletionClient(
                model=settings.UI_TARS_MODEL,
                api_key=settings.UI_TARS_API_KEY,
                base_url=settings.UI_TARS_BASE_URL,
                model_info={
                    "vision": True,
                    "function_calling": True,
                    "json_output": True,
                    "structured_output": True,
                    "family": "doubao",
                    "multiple_system_messages": True
                }
            )
            logger.info("âœ… UI-TARSå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ UI-TARSå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨QWen-VLå¤‡ç”¨")
            return get_qwenvl_model_client()
    return _uitars_model_client

# æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨
def get_optimal_model_for_task(task_type: str) -> OpenAIChatCompletionClient:
    """æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼ˆå¸¦å¯†é’¥å¯ç”¨æ€§å›é€€ï¼‰ã€‚

    ç­–ç•¥ï¼š
    - playwright_generation / code_generation: DeepSeek > QWen-VL > GLM
    - ui/image/page ç›¸å…³: QWen-VL > GLM > DeepSeek
    - complex/multimodal: GLM > QWen-VL > DeepSeek
    - é»˜è®¤: QWen-VL > GLM > DeepSeek
    """

    status = get_model_config_status()

    def available(name: str) -> bool:
        return bool({
            'qwen_vl': status.get('qwen_vl') or status.get('qwen'),
            'glm': status.get('glm'),
            'deepseek': status.get('deepseek')
        }.get(name, False))

    order_map = {
        'playwright_generation': ['deepseek', 'qwen_vl', 'glm'],
        'code_generation': ['deepseek', 'qwen_vl', 'glm'],
        'ui_analysis': ['qwen_vl', 'glm', 'deepseek'],
        'image_analysis': ['qwen_vl', 'glm', 'deepseek'],
        'page_analysis': ['qwen_vl', 'glm', 'deepseek'],
        'element_recognition': ['qwen_vl', 'glm', 'deepseek'],
        'visual_testing': ['qwen_vl', 'glm', 'deepseek'],
        'complex_analysis': ['glm', 'qwen_vl', 'deepseek'],
        'multimodal_reasoning': ['glm', 'qwen_vl', 'deepseek'],
        'business_analysis': ['glm', 'qwen_vl', 'deepseek'],
        'default': ['qwen_vl', 'glm', 'deepseek']
    }

    for name in order_map.get(task_type, order_map['default']):
        if name == 'deepseek' and available('deepseek'):
            logger.info(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {task_type} -> é€‰æ‹©æ¨¡å‹: DeepSeek(é«˜æ€§ä»·æ¯”)")
            return get_deepseek_model_client()
        if name == 'qwen_vl' and available('qwen_vl'):
            logger.info(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {task_type} -> é€‰æ‹©æ¨¡å‹: QWen-VL(æœ€ä½³)")
            return get_qwenvl_model_client()
        if name == 'glm' and available('glm'):
            logger.info(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {task_type} -> é€‰æ‹©æ¨¡å‹: GLM-4V(èƒ½åŠ›å¼º)")
            return get_glm_model_client()

    # è‹¥å‡ä¸å¯ç”¨ï¼ŒæŠ›é”™ä»¥ä¾¿å‰ç«¯æç¤ºé…ç½®å¯†é’¥
    raise RuntimeError("æœªæ£€æµ‹åˆ°å¯ç”¨çš„AIæ¨¡å‹å¯†é’¥ï¼Œè¯·åœ¨åç«¯ç¯å¢ƒå˜é‡ä¸­é…ç½®è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆå¯†é’¥ï¼ˆQWEN_VL_API_KEY/GLM_API_KEY/DEEPSEEK_API_KEYï¼‰ã€‚")

# æ¨¡å‹é…ç½®çŠ¶æ€æ£€æŸ¥å‡½æ•°
def get_model_config_status() -> Dict[str, bool]:
    """è·å–æ‰€æœ‰AIæ¨¡å‹çš„é…ç½®çŠ¶æ€"""
    return {
        "qwen_vl": bool(settings.QWEN_VL_API_KEY and settings.QWEN_VL_API_KEY.strip() and not settings.QWEN_VL_API_KEY.startswith('your-')),
        "qwen": bool(settings.QWEN_API_KEY and settings.QWEN_API_KEY.strip() and not settings.QWEN_API_KEY.startswith('your-')),
        "glm": bool(settings.GLM_API_KEY and settings.GLM_API_KEY.strip() and not settings.GLM_API_KEY.startswith('your-')),
        "deepseek": bool(settings.DEEPSEEK_API_KEY and settings.DEEPSEEK_API_KEY.strip() and not settings.DEEPSEEK_API_KEY.startswith('your-')),
        "uitars": bool(settings.UI_TARS_API_KEY and settings.UI_TARS_API_KEY.strip() and not settings.UI_TARS_API_KEY.startswith('your-')),
        "openai": bool(settings.OPENAI_API_KEY and settings.OPENAI_API_KEY.strip() and not settings.OPENAI_API_KEY.startswith('your-')),
        "gemini": bool(settings.GEMINI_API_KEY and settings.GEMINI_API_KEY.strip() and not settings.GEMINI_API_KEY.startswith('your-'))
    }

# å‘åå…¼å®¹çš„æ¨¡å‹è·å–å‡½æ•°
def get_model_client(model_type: str = "auto") -> OpenAIChatCompletionClient:
    """è·å–æ¨¡å‹å®¢æˆ·ç«¯ - æ”¯æŒè‡ªåŠ¨é€‰æ‹©å’Œæ‰‹åŠ¨æŒ‡å®š"""
    
    if model_type == "auto":
        return get_qwenvl_model_client()  # é»˜è®¤ä½¿ç”¨æœ€ä½³æ¨¡å‹
    elif model_type == "qwen" or model_type == "qwenvl":
        return get_qwenvl_model_client()
    elif model_type == "deepseek":
        return get_deepseek_model_client()
    elif model_type == "glm":
        return get_glm_model_client()
    elif model_type == "uitars":
        return get_uitars_model_client()
    else:
        logger.warning(f"âš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}ï¼Œä½¿ç”¨é»˜è®¤QWen-VL")
        return get_qwenvl_model_client()