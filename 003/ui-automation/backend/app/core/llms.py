"""
å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯é…ç½®
æ ¹æ®éªŒè¯ç»“æœä¼˜åŒ–æ¨¡å‹é€‰æ‹©å’Œä½¿ç”¨ï¼š
- QWen-VL: UIè‡ªåŠ¨åŒ–æœ€ä½³ï¼Œè§†è§‰ç†è§£ã€å›¾ç‰‡å¤„ç†
- GLM-4V: å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¤æ‚åœºæ™¯åˆ†æ  
- DeepSeek: ä»£ç ç”Ÿæˆã€æ–‡æœ¬å¤„ç†ï¼Œæ€§ä»·æ¯”æé«˜
"""
from typing import Optional, Dict, Any, List
from abc import ABC, abstractmethod

from openai import AsyncOpenAI

from autogen_ext.models.openai import OpenAIChatCompletionClient
from loguru import logger

from app.core.config import settings

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_deepseek_model_client = None
_qwenvl_model_client = None
_glm_model_client = None
_uitars_model_client = None

def get_deepseek_model_client() -> OpenAIChatCompletionClient:
    """è·å–DeepSeekå®¢æˆ·ç«¯ - ä¸“ç”¨äºä»£ç ç”Ÿæˆã€æ–‡æœ¬å¤„ç† (æ€§ä»·æ¯”æé«˜)"""
    global _deepseek_model_client
    if _deepseek_model_client is None:
        logger.info("ğŸ’° åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ - ä»£ç ç”Ÿæˆä¸“ç”¨")
        _deepseek_model_client = OpenAIChatCompletionClient(
            model=settings.DEEPSEEK_MODEL,
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL,
            model_info={
                "vision": False,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,
                "family": "deepseek",
                "multiple_system_messages": True
            }
        )
    return _deepseek_model_client

def get_qwenvl_model_client() -> OpenAIChatCompletionClient:
    """è·å–QWen-VLå®¢æˆ·ç«¯ - æœ€ä½³æ¨èï¼Œä¸“ç”¨äºUIè‡ªåŠ¨åŒ–ã€å›¾ç‰‡åˆ†æ"""
    global _qwenvl_model_client
    if _qwenvl_model_client is None:
        logger.info("ğŸ¥‡ åˆå§‹åŒ–QWen-VLå®¢æˆ·ç«¯ - UIè‡ªåŠ¨åŒ–æœ€ä½³é€‰æ‹©")
        _qwenvl_model_client = OpenAIChatCompletionClient(
            model=settings.QWEN_VL_MODEL,
            api_key=settings.QWEN_VL_API_KEY,
            base_url=settings.QWEN_VL_BASE_URL,
            model_info={
                "vision": True,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,
                "family": "qwen",
                "multiple_system_messages": True
            }
        )
    return _qwenvl_model_client

def get_glm_model_client() -> OpenAIChatCompletionClient:
    """è·å–GLM-4Vå®¢æˆ·ç«¯ - ä¸“ç”¨äºå¤æ‚å¤šæ¨¡æ€ä»»åŠ¡"""
    global _glm_model_client
    if _glm_model_client is None:
        logger.info("ğŸ¥ˆ åˆå§‹åŒ–GLM-4Vå®¢æˆ·ç«¯ - å¤šæ¨¡æ€ä»»åŠ¡ä¸“ç”¨")
        _glm_model_client = OpenAIChatCompletionClient(
            model=settings.GLM_MODEL,
            api_key=settings.GLM_API_KEY,
            base_url=settings.GLM_BASE_URL,
            model_info={
                "vision": True,
                "function_calling": True,
                "json_output": True,
                "structured_output": True,
                "family": "glm",
                "multiple_system_messages": True
            }
        )
    return _glm_model_client

def get_uitars_model_client() -> OpenAIChatCompletionClient:
    """è·å–UI-TARSå®¢æˆ·ç«¯ - è±†åŒ…UIè‡ªåŠ¨åŒ–ä¸“ç”¨ï¼ˆå½“å‰ä¸å¯ç”¨ï¼‰"""
    global _uitars_model_client
    if _uitars_model_client is None:
        logger.warning("âš ï¸ UI-TARSå®¢æˆ·ç«¯å½“å‰ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
        # ä½¿ç”¨QWen-VLä½œä¸ºå¤‡ç”¨
        return get_qwenvl_model_client()
    return _uitars_model_client

# æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨
def get_optimal_model_for_task(task_type: str) -> OpenAIChatCompletionClient:
    """æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
    
    task_model_mapping = {
        # UIç›¸å…³ä»»åŠ¡ - ä½¿ç”¨QWen-VL (æœ€ä½³)
        "ui_analysis": get_qwenvl_model_client,
        "image_analysis": get_qwenvl_model_client,
        "page_analysis": get_qwenvl_model_client,
        "element_recognition": get_qwenvl_model_client,
        "visual_testing": get_qwenvl_model_client,
        
        # ä»£ç ç”Ÿæˆä»»åŠ¡ - ä½¿ç”¨DeepSeek (æ€§ä»·æ¯”æé«˜)
        "code_generation": get_deepseek_model_client,
        "playwright_generation": get_deepseek_model_client,
        "yaml_generation": get_deepseek_model_client,
        "test_script": get_deepseek_model_client,
        "text_processing": get_deepseek_model_client,
        
        # å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ - ä½¿ç”¨GLM-4V (èƒ½åŠ›å¼º)
        "complex_analysis": get_glm_model_client,
        "multimodal_reasoning": get_glm_model_client,
        "business_analysis": get_glm_model_client,
        
        # é»˜è®¤é€‰æ‹©
        "default": get_qwenvl_model_client
    }
    
    selected_model_fn = task_model_mapping.get(task_type, task_model_mapping["default"])
    model_name = {
        get_qwenvl_model_client: "QWen-VL(æœ€ä½³)",
        get_deepseek_model_client: "DeepSeek(é«˜æ€§ä»·æ¯”)", 
        get_glm_model_client: "GLM-4V(èƒ½åŠ›å¼º)"
    }.get(selected_model_fn, "æœªçŸ¥æ¨¡å‹")
    
    logger.info(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {task_type} -> é€‰æ‹©æ¨¡å‹: {model_name}")
    return selected_model_fn()

# å‘åå…¼å®¹çš„æ¨¡å‹è·å–å‡½æ•°
def get_model_client(model_type: str = "auto") -> OpenAIChatCompletionClient:
    """è·å–æ¨¡å‹å®¢æˆ·ç«¯ - æ”¯æŒè‡ªåŠ¨é€‰æ‹©å’Œæ‰‹åŠ¨æŒ‡å®š"""
    
    if model_type == "auto":
        return get_qwenvl_model_client()  # é»˜è®¤ä½¿ç”¨æœ€ä½³æ¨¡å‹
    elif model_type == "qwen" or model_type == "qwenvl":
        return get_qwenvl_model_client()
    elif model_type == "deepseek":
        return get_deepseek_model_client()
    elif model_type == "glm":
        return get_glm_model_client()
    elif model_type == "uitars":
        return get_uitars_model_client()
    else:
        logger.warning(f"âš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}ï¼Œä½¿ç”¨é»˜è®¤QWen-VL")
        return get_qwenvl_model_client()